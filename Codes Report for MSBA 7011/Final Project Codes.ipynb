{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import shutil\n",
    "#import matplotlib.pyplot as plt\n",
    "#from matplotlib import ticker\n",
    "#import seaborn as sns\n",
    "#%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prefix_dir = 'input/train/'\n",
    "\n",
    "ROWS = 64\n",
    "COLS = 64\n",
    "CHANNELS = 3\n",
    "\n",
    "images = [i for i in os.listdir(prefix_dir)] # use this for full dataset\n",
    "dogs =   [i for i in os.listdir(prefix_dir) if 'dog' in i]\n",
    "cats =   [i for i in os.listdir(prefix_dir) if 'cat' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(left_perc,train_perc):\n",
    "    random.shuffle(images)\n",
    "    left_images = images[:int(len(images)*left_perc)]\n",
    "    train_images = left_images[:int(len(left_images)*train_perc)]\n",
    "    test_images = left_images[int(len(left_images)*train_perc):]\n",
    "    random.shuffle(cats)\n",
    "    random.shuffle(dogs)\n",
    "    left_cats = cats[:int(len(cats)*left_perc)]\n",
    "    left_dogs = dogs[:int(len(dogs)*left_perc)]\n",
    "    train_num = int(len(left_cats)*train_perc)\n",
    "    train_cats = left_cats[:train_num]\n",
    "    train_dogs = left_dogs[:train_num]\n",
    "    test_cats = left_cats[train_num:]\n",
    "    test_dogs = left_dogs[train_num:]\n",
    "    train_images = train_cats+train_dogs\n",
    "    test_images = test_cats+test_dogs\n",
    "    return train_images,test_images\n",
    "\n",
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR) #cv2.IMREAD_GRAYSCALE\n",
    "    img = img[:,:,::-1]/255\n",
    "    return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def prep_data(images,prefix_dir):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count,ROWS,COLS,CHANNELS))\n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(prefix_dir+image_file)\n",
    "        data[i] = image\n",
    "        if (i+1)%2000 == 0: print('Processed {} of {}'.format(i+1, count))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 of 4000\n",
      "Processed 4000 of 4000\n"
     ]
    }
   ],
   "source": [
    "# We just use [left_perc] of our total dataset.\n",
    "left_perc = 0.2\n",
    "train_perc = 0.8\n",
    "\n",
    "train_images,test_images = get_data(left_perc,train_perc)\n",
    "train_x = prep_data(train_images,prefix_dir)\n",
    "test_x = prep_data(test_images,prefix_dir)\n",
    "\n",
    "train_y = [1 if x[:3]=='dog' else 0 for x in train_images]\n",
    "test_y = [1 if x[:3]=='dog' else 0 for x in test_images]\n",
    "train_y = to_categorical(train_y)\n",
    "test_y = to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (4000, 64, 64, 3)\n",
      "test_x shape: (1000, 64, 64, 3)\n",
      "train_y shape: (4000, 2)\n",
      "test_y shape: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x shape: {}\".format(train_x.shape))\n",
    "print(\"test_x shape: {}\".format(test_x.shape))\n",
    "print('train_y shape: {}'.format(train_y.shape))\n",
    "print('test_y shape: {}'.format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up 4 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model  \n",
    "from keras.layers import Dense,Flatten,Dropout,Input,concatenate,BatchNormalization,Activation,ZeroPadding2D,add\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D  \n",
    "  \n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D,AveragePooling2D  \n",
    "from keras.optimizers import SGD  \n",
    "import numpy as np  \n",
    "seed = 888 \n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model1: LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 60, 60, 32)        2432      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1081700   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,135,598\n",
      "Trainable params: 1,135,598\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()  \n",
    "model1.add(Conv2D(32,(5,5),strides=(1,1),input_shape=(64,64,3),padding='valid',activation='relu',kernel_initializer='uniform')) \n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "model1.add(Conv2D(64,(5,5),strides=(1,1),padding='valid',activation='relu',kernel_initializer='uniform'))  \n",
    "model1.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "model1.add(Flatten())  \n",
    "model1.add(Dense(100,activation='relu'))  \n",
    "model1.add(Dense(2,activation='softmax'))  \n",
    "model1.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])  \n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.6921 - acc: 0.5203 - val_loss: 0.6883 - val_acc: 0.5080\n",
      "Epoch 2/20\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.6866 - acc: 0.5520 - val_loss: 0.6835 - val_acc: 0.5210\n",
      "Epoch 3/20\n",
      "4000/4000 [==============================] - 42s 10ms/step - loss: 0.6823 - acc: 0.5705 - val_loss: 0.6756 - val_acc: 0.6170\n",
      "Epoch 4/20\n",
      "4000/4000 [==============================] - 45s 11ms/step - loss: 0.6765 - acc: 0.5800 - val_loss: 0.6722 - val_acc: 0.5590\n",
      "Epoch 5/20\n",
      "4000/4000 [==============================] - 48s 12ms/step - loss: 0.6744 - acc: 0.5773 - val_loss: 0.6656 - val_acc: 0.5960\n",
      "Epoch 6/20\n",
      "4000/4000 [==============================] - 44s 11ms/step - loss: 0.6674 - acc: 0.5910 - val_loss: 0.6549 - val_acc: 0.6360\n",
      "Epoch 7/20\n",
      "4000/4000 [==============================] - 46s 11ms/step - loss: 0.6594 - acc: 0.6123 - val_loss: 0.6672 - val_acc: 0.5750\n",
      "Epoch 8/20\n",
      "4000/4000 [==============================] - 45s 11ms/step - loss: 0.6546 - acc: 0.6160 - val_loss: 0.6814 - val_acc: 0.5470\n",
      "Epoch 9/20\n",
      "4000/4000 [==============================] - 47s 12ms/step - loss: 0.6500 - acc: 0.6162 - val_loss: 0.6387 - val_acc: 0.6490\n",
      "Epoch 10/20\n",
      "4000/4000 [==============================] - 48s 12ms/step - loss: 0.6400 - acc: 0.6353 - val_loss: 0.6320 - val_acc: 0.6640\n",
      "Epoch 11/20\n",
      "4000/4000 [==============================] - 52s 13ms/step - loss: 0.6321 - acc: 0.6338 - val_loss: 0.6255 - val_acc: 0.6570\n",
      "Epoch 12/20\n",
      "4000/4000 [==============================] - 46s 11ms/step - loss: 0.6244 - acc: 0.6558 - val_loss: 0.6151 - val_acc: 0.6770\n",
      "Epoch 13/20\n",
      "4000/4000 [==============================] - 47s 12ms/step - loss: 0.6126 - acc: 0.6745 - val_loss: 0.6533 - val_acc: 0.5860\n",
      "Epoch 14/20\n",
      "4000/4000 [==============================] - 48s 12ms/step - loss: 0.6013 - acc: 0.6720 - val_loss: 0.5996 - val_acc: 0.6840\n",
      "Epoch 15/20\n",
      "4000/4000 [==============================] - 48s 12ms/step - loss: 0.5903 - acc: 0.6815 - val_loss: 0.6398 - val_acc: 0.6080\n",
      "Epoch 16/20\n",
      "4000/4000 [==============================] - 41s 10ms/step - loss: 0.5792 - acc: 0.6960 - val_loss: 0.5914 - val_acc: 0.6950\n",
      "Epoch 17/20\n",
      "4000/4000 [==============================] - 47s 12ms/step - loss: 0.5743 - acc: 0.6970 - val_loss: 0.5913 - val_acc: 0.6880\n",
      "Epoch 18/20\n",
      "4000/4000 [==============================] - 42s 11ms/step - loss: 0.5645 - acc: 0.7120 - val_loss: 0.5884 - val_acc: 0.7010\n",
      "Epoch 19/20\n",
      "4000/4000 [==============================] - 49s 12ms/step - loss: 0.5582 - acc: 0.7170 - val_loss: 0.5808 - val_acc: 0.7020\n",
      "Epoch 20/20\n",
      "4000/4000 [==============================] - 51s 13ms/step - loss: 0.5503 - acc: 0.7158 - val_loss: 0.6082 - val_acc: 0.6450\n"
     ]
    }
   ],
   "source": [
    "model1.fit(train_x,train_y,batch_size=50,epochs=20,verbose=1,validation_data=(test_x,test_y))\n",
    "model1.save('model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model2: AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 27, 27, 96)        34944     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 6, 6, 384)         885120    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4096)              4198400   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 24,735,106\n",
      "Trainable params: 24,735,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()  \n",
    "#model2.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(227,227,3),padding='valid',activation='relu',kernel_initializer='uniform'))\n",
    "model2.add(Conv2D(96,(11,11),strides=(2,2),input_shape=(64,64,3),padding='valid',activation='relu',kernel_initializer='uniform'))\n",
    "model2.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model2.add(Conv2D(256,(5,5),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model2.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model2.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model2.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model2.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model2.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model2.add(Flatten())  \n",
    "model2.add(Dense(4096,activation='relu'))  \n",
    "model2.add(Dropout(0.5))  \n",
    "model2.add(Dense(4096,activation='relu'))  \n",
    "model2.add(Dropout(0.5))  \n",
    "model2.add(Dense(2,activation='softmax'))  \n",
    "model2.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])  \n",
    "model2.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "4000/4000 [==============================] - 169s 42ms/step - loss: 0.7092 - acc: 0.5178 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "4000/4000 [==============================] - 168s 42ms/step - loss: 0.6953 - acc: 0.5283 - val_loss: 0.6900 - val_acc: 0.5020\n",
      "Epoch 3/20\n",
      "4000/4000 [==============================] - 176s 44ms/step - loss: 0.6865 - acc: 0.5558 - val_loss: 0.7353 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "4000/4000 [==============================] - 171s 43ms/step - loss: 0.6923 - acc: 0.5510 - val_loss: 0.6758 - val_acc: 0.5410\n",
      "Epoch 5/20\n",
      "4000/4000 [==============================] - 172s 43ms/step - loss: 0.6890 - acc: 0.5510 - val_loss: 0.6700 - val_acc: 0.5730\n",
      "Epoch 6/20\n",
      "4000/4000 [==============================] - 173s 43ms/step - loss: 0.6918 - acc: 0.5560 - val_loss: 0.6644 - val_acc: 0.5880\n",
      "Epoch 7/20\n",
      "4000/4000 [==============================] - 169s 42ms/step - loss: 0.6964 - acc: 0.5530 - val_loss: 0.6692 - val_acc: 0.5850\n",
      "Epoch 8/20\n",
      "4000/4000 [==============================] - 172s 43ms/step - loss: 0.6936 - acc: 0.5500 - val_loss: 0.6640 - val_acc: 0.6170\n",
      "Epoch 9/20\n",
      "4000/4000 [==============================] - 171s 43ms/step - loss: 0.6939 - acc: 0.5428 - val_loss: 0.6842 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "4000/4000 [==============================] - 157s 39ms/step - loss: 0.6905 - acc: 0.5350 - val_loss: 0.6809 - val_acc: 0.5120\n",
      "Epoch 11/20\n",
      "4000/4000 [==============================] - 157s 39ms/step - loss: 0.6911 - acc: 0.5345 - val_loss: 0.6806 - val_acc: 0.5650\n",
      "Epoch 12/20\n",
      "4000/4000 [==============================] - 157s 39ms/step - loss: 0.6932 - acc: 0.5207 - val_loss: 0.6879 - val_acc: 0.5440\n",
      "Epoch 13/20\n",
      "4000/4000 [==============================] - 157s 39ms/step - loss: 0.6872 - acc: 0.5457 - val_loss: 0.6780 - val_acc: 0.5440\n",
      "Epoch 14/20\n",
      "4000/4000 [==============================] - 157s 39ms/step - loss: 0.6945 - acc: 0.5365 - val_loss: 0.6855 - val_acc: 0.5620\n",
      "Epoch 15/20\n",
      "4000/4000 [==============================] - 157s 39ms/step - loss: 0.6991 - acc: 0.5318 - val_loss: 0.6989 - val_acc: 0.4930\n",
      "Epoch 16/20\n",
      "4000/4000 [==============================] - 158s 39ms/step - loss: 0.6957 - acc: 0.5093 - val_loss: 0.6915 - val_acc: 0.5020\n",
      "Epoch 17/20\n",
      "4000/4000 [==============================] - 157s 39ms/step - loss: 0.6969 - acc: 0.5030 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "4000/4000 [==============================] - 157s 39ms/step - loss: 0.6933 - acc: 0.5007 - val_loss: 0.6932 - val_acc: 0.5010\n",
      "Epoch 19/20\n",
      "4000/4000 [==============================] - 157s 39ms/step - loss: 0.6938 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.5090\n",
      "Epoch 20/20\n",
      "4000/4000 [==============================] - 157s 39ms/step - loss: 0.6932 - acc: 0.4837 - val_loss: 0.6931 - val_acc: 0.5160\n"
     ]
    }
   ],
   "source": [
    "model2.fit(train_x,train_y,batch_size=50,epochs=20,verbose=1,validation_data=(test_x,test_y))\n",
    "model2.save('model2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model3: ZFNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 29, 29, 96)        14208     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 256)         614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 3, 3, 384)         885120    \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 3, 3, 256)         884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 21,568,642\n",
      "Trainable params: 21,568,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()  \n",
    "model3.add(Conv2D(96,(7,7),strides=(2,2),input_shape=(64,64,3),padding='valid',activation='relu',kernel_initializer='uniform'))  \n",
    "model3.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model3.add(Conv2D(256,(5,5),strides=(2,2),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model3.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model3.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model3.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model3.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model3.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model3.add(Flatten())  \n",
    "model3.add(Dense(4096,activation='relu'))  \n",
    "model3.add(Dropout(0.5))  \n",
    "model3.add(Dense(4096,activation='relu'))  \n",
    "model3.add(Dropout(0.5))  \n",
    "#model3.add(Dense(1000,activation='softmax'))\n",
    "model3.add(Dense(2,activation='softmax'))\n",
    "model3.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])  \n",
    "model3.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.6960 - acc: 0.5072 - val_loss: 0.6890 - val_acc: 0.6140\n",
      "Epoch 2/20\n",
      "4000/4000 [==============================] - 82s 21ms/step - loss: 0.6914 - acc: 0.5213 - val_loss: 0.6866 - val_acc: 0.5090\n",
      "Epoch 3/20\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.6914 - acc: 0.5270 - val_loss: 0.6811 - val_acc: 0.6340\n",
      "Epoch 4/20\n",
      "4000/4000 [==============================] - 82s 21ms/step - loss: 0.6849 - acc: 0.5568 - val_loss: 0.6734 - val_acc: 0.6240\n",
      "Epoch 5/20\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.6781 - acc: 0.5720 - val_loss: 0.6638 - val_acc: 0.6390\n",
      "Epoch 6/20\n",
      "4000/4000 [==============================] - 84s 21ms/step - loss: 0.6746 - acc: 0.5730 - val_loss: 0.6527 - val_acc: 0.6430\n",
      "Epoch 7/20\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.6702 - acc: 0.5910 - val_loss: 0.6603 - val_acc: 0.5940\n",
      "Epoch 8/20\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.6705 - acc: 0.5918 - val_loss: 0.6480 - val_acc: 0.6270\n",
      "Epoch 9/20\n",
      "4000/4000 [==============================] - 82s 21ms/step - loss: 0.6666 - acc: 0.5983 - val_loss: 0.6700 - val_acc: 0.5880\n",
      "Epoch 10/20\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.6751 - acc: 0.5925 - val_loss: 0.6333 - val_acc: 0.6580\n",
      "Epoch 11/20\n",
      "4000/4000 [==============================] - 82s 21ms/step - loss: 0.6650 - acc: 0.5978 - val_loss: 0.6434 - val_acc: 0.6180\n",
      "Epoch 12/20\n",
      "4000/4000 [==============================] - 82s 21ms/step - loss: 0.6658 - acc: 0.5913 - val_loss: 0.6419 - val_acc: 0.6300\n",
      "Epoch 13/20\n",
      "4000/4000 [==============================] - 82s 21ms/step - loss: 0.6710 - acc: 0.5855 - val_loss: 0.6388 - val_acc: 0.6360\n",
      "Epoch 14/20\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.6665 - acc: 0.6048 - val_loss: 0.6439 - val_acc: 0.6340\n",
      "Epoch 15/20\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.6692 - acc: 0.5958 - val_loss: 0.6407 - val_acc: 0.6510\n",
      "Epoch 16/20\n",
      "4000/4000 [==============================] - 82s 21ms/step - loss: 0.6702 - acc: 0.5913 - val_loss: 0.6318 - val_acc: 0.6400\n",
      "Epoch 17/20\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.6783 - acc: 0.5943 - val_loss: 0.6380 - val_acc: 0.6330\n",
      "Epoch 18/20\n",
      "4000/4000 [==============================] - 83s 21ms/step - loss: 0.6752 - acc: 0.5965 - val_loss: 0.6401 - val_acc: 0.6230\n",
      "Epoch 19/20\n",
      "4000/4000 [==============================] - 82s 21ms/step - loss: 0.6815 - acc: 0.5778 - val_loss: 0.6669 - val_acc: 0.5800\n",
      "Epoch 20/20\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.6876 - acc: 0.5668 - val_loss: 0.6500 - val_acc: 0.6390\n"
     ]
    }
   ],
   "source": [
    "model3.fit(train_x,train_y,batch_size=50,epochs=20,verbose=1,validation_data=(test_x,test_y))\n",
    "model3.save('model3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model4: VGG-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_23 (Conv2D)           (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 32, 32, 128)       49280     \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 34,562,626\n",
      "Trainable params: 34,562,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()  \n",
    "model4.add(Conv2D(64,(3,3),strides=(1,1),input_shape=(64,64,3),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model4.add(Conv2D(64,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model4.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "model4.add(Conv2D(128,(3,2),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model4.add(Conv2D(128,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model4.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "model4.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model4.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model4.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "model4.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model4.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model4.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "model4.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model4.add(Conv2D(512,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model4.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "model4.add(Flatten())  \n",
    "model4.add(Dense(4096,activation='relu'))  \n",
    "model4.add(Dropout(0.5))  \n",
    "model4.add(Dense(4096,activation='relu'))  \n",
    "model4.add(Dropout(0.5))  \n",
    "model4.add(Dense(2,activation='softmax'))  \n",
    "model4.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])  \n",
    "model4.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "4000/4000 [==============================] - 544s 136ms/step - loss: 0.6930 - acc: 0.5110 - val_loss: 0.6928 - val_acc: 0.5360\n",
      "Epoch 2/20\n",
      "4000/4000 [==============================] - 538s 134ms/step - loss: 0.6930 - acc: 0.5038 - val_loss: 0.6923 - val_acc: 0.5270\n",
      "Epoch 3/20\n",
      "4000/4000 [==============================] - 538s 134ms/step - loss: 0.6924 - acc: 0.5212 - val_loss: 0.6921 - val_acc: 0.5200\n",
      "Epoch 4/20\n",
      "4000/4000 [==============================] - 540s 135ms/step - loss: 0.6926 - acc: 0.5150 - val_loss: 0.6916 - val_acc: 0.5010\n",
      "Epoch 5/20\n",
      "4000/4000 [==============================] - 537s 134ms/step - loss: 0.6917 - acc: 0.5307 - val_loss: 0.6907 - val_acc: 0.5290\n",
      "Epoch 6/20\n",
      "4000/4000 [==============================] - 538s 135ms/step - loss: 0.6914 - acc: 0.5260 - val_loss: 0.6895 - val_acc: 0.6060\n",
      "Epoch 7/20\n",
      "4000/4000 [==============================] - 538s 134ms/step - loss: 0.6910 - acc: 0.5348 - val_loss: 0.6915 - val_acc: 0.5010\n",
      "Epoch 8/20\n",
      "4000/4000 [==============================] - 538s 134ms/step - loss: 0.6900 - acc: 0.5445 - val_loss: 0.6865 - val_acc: 0.6100\n",
      "Epoch 9/20\n",
      "4000/4000 [==============================] - 538s 134ms/step - loss: 0.6902 - acc: 0.5425 - val_loss: 0.6918 - val_acc: 0.5010\n",
      "Epoch 10/20\n",
      "4000/4000 [==============================] - 538s 134ms/step - loss: 0.6886 - acc: 0.5423 - val_loss: 0.6864 - val_acc: 0.5160\n",
      "Epoch 11/20\n",
      "4000/4000 [==============================] - 539s 135ms/step - loss: 0.6882 - acc: 0.5478 - val_loss: 0.6997 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "4000/4000 [==============================] - 538s 135ms/step - loss: 0.6869 - acc: 0.5435 - val_loss: 0.6789 - val_acc: 0.5840\n",
      "Epoch 13/20\n",
      "4000/4000 [==============================] - 537s 134ms/step - loss: 0.6866 - acc: 0.5490 - val_loss: 0.6750 - val_acc: 0.6260\n",
      "Epoch 14/20\n",
      "4000/4000 [==============================] - 540s 135ms/step - loss: 0.6842 - acc: 0.5465 - val_loss: 0.6744 - val_acc: 0.5830\n",
      "Epoch 15/20\n",
      "4000/4000 [==============================] - 539s 135ms/step - loss: 0.6828 - acc: 0.5575 - val_loss: 0.6842 - val_acc: 0.5350\n",
      "Epoch 16/20\n",
      "4000/4000 [==============================] - 542s 135ms/step - loss: 0.6832 - acc: 0.5560 - val_loss: 0.6752 - val_acc: 0.5770\n",
      "Epoch 17/20\n",
      "4000/4000 [==============================] - 536s 134ms/step - loss: 0.6814 - acc: 0.5635 - val_loss: 0.6779 - val_acc: 0.5500\n",
      "Epoch 18/20\n",
      "4000/4000 [==============================] - 547s 137ms/step - loss: 0.6781 - acc: 0.5723 - val_loss: 0.6610 - val_acc: 0.6180\n",
      "Epoch 19/20\n",
      "4000/4000 [==============================] - 565s 141ms/step - loss: 0.6783 - acc: 0.5658 - val_loss: 0.6568 - val_acc: 0.6220\n",
      "Epoch 20/20\n",
      "4000/4000 [==============================] - 654s 163ms/step - loss: 0.6751 - acc: 0.5713 - val_loss: 0.6510 - val_acc: 0.6450\n"
     ]
    }
   ],
   "source": [
    "model4.fit(train_x,train_y,batch_size=50,epochs=20,verbose=1,validation_data=(test_x,test_y))\n",
    "model4.save('model4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Conclusion on LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def start_out():\n",
    "    model0 = Sequential()  \n",
    "    model0.add(Conv2D(32,(5,5),strides=(1,1),input_shape=(64,64,3),padding='valid',activation='relu',kernel_initializer='uniform')) \n",
    "    model0.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "    model0.add(Conv2D(64,(5,5),strides=(1,1),padding='valid',activation='relu',kernel_initializer='uniform'))  \n",
    "    model0.add(MaxPooling2D(pool_size=(2,2)))  \n",
    "    model0.add(Flatten())  \n",
    "    model0.add(Dense(100,activation='relu'))  \n",
    "    model0.add(Dense(2,activation='softmax'))  \n",
    "    model0.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])  \n",
    "    model0.summary()\n",
    "    return model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000 of 20000\n",
      "Processed 4000 of 20000\n",
      "Processed 6000 of 20000\n",
      "Processed 8000 of 20000\n",
      "Processed 10000 of 20000\n",
      "Processed 12000 of 20000\n",
      "Processed 14000 of 20000\n",
      "Processed 16000 of 20000\n",
      "Processed 18000 of 20000\n",
      "Processed 20000 of 20000\n",
      "Processed 2000 of 5000\n",
      "Processed 4000 of 5000\n",
      "train shape: (20000, 64, 64, 3) (20000, 2)\n",
      "test shape: (5000, 64, 64, 3) (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_images,test_images = get_data(1,0.8)\n",
    "train_x = prep_data(train_images,prefix_dir)\n",
    "test_x = prep_data(test_images,prefix_dir)\n",
    "train_y = [1 if x[:3]=='dog' else 0 for x in train_images]\n",
    "test_y = [1 if x[:3]=='dog' else 0 for x in test_images]\n",
    "train_y = to_categorical(train_y)\n",
    "test_y = to_categorical(test_y)\n",
    "print('train shape:',train_x.shape,train_y.shape)\n",
    "print('test shape:',test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 60, 60, 32)        2432      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               1081700   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,135,598\n",
      "Trainable params: 1,135,598\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 188s 9ms/step - loss: 0.6846 - acc: 0.5496 - val_loss: 0.6709 - val_acc: 0.6208\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 186s 9ms/step - loss: 0.6672 - acc: 0.5985 - val_loss: 0.6469 - val_acc: 0.6490\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 187s 9ms/step - loss: 0.6504 - acc: 0.6198 - val_loss: 0.6525 - val_acc: 0.6022\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 184s 9ms/step - loss: 0.6299 - acc: 0.6447 - val_loss: 0.6068 - val_acc: 0.6794\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 185s 9ms/step - loss: 0.6049 - acc: 0.6742 - val_loss: 0.5878 - val_acc: 0.6850\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 185s 9ms/step - loss: 0.5772 - acc: 0.7002 - val_loss: 0.5554 - val_acc: 0.7202\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 186s 9ms/step - loss: 0.5561 - acc: 0.7175 - val_loss: 0.5316 - val_acc: 0.7446\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 186s 9ms/step - loss: 0.5350 - acc: 0.7338 - val_loss: 0.5201 - val_acc: 0.7454\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 184s 9ms/step - loss: 0.5165 - acc: 0.7437 - val_loss: 0.5104 - val_acc: 0.7472\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 186s 9ms/step - loss: 0.4983 - acc: 0.7578 - val_loss: 0.4965 - val_acc: 0.7552\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 186s 9ms/step - loss: 0.4817 - acc: 0.7681 - val_loss: 0.4822 - val_acc: 0.7714\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 185s 9ms/step - loss: 0.4676 - acc: 0.7781 - val_loss: 0.4996 - val_acc: 0.7518\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 192s 10ms/step - loss: 0.4548 - acc: 0.7875 - val_loss: 0.4619 - val_acc: 0.7860\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 187s 9ms/step - loss: 0.4441 - acc: 0.7947 - val_loss: 0.4541 - val_acc: 0.7864\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 179s 9ms/step - loss: 0.4333 - acc: 0.8021 - val_loss: 0.4464 - val_acc: 0.8000\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 170s 8ms/step - loss: 0.4224 - acc: 0.8068 - val_loss: 0.4690 - val_acc: 0.7774\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 165s 8ms/step - loss: 0.4109 - acc: 0.8121 - val_loss: 0.4373 - val_acc: 0.8066\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 187s 9ms/step - loss: 0.3994 - acc: 0.8181 - val_loss: 0.4371 - val_acc: 0.8032\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 222s 11ms/step - loss: 0.3892 - acc: 0.8254 - val_loss: 0.4710 - val_acc: 0.7716\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 211s 11ms/step - loss: 0.3758 - acc: 0.8332 - val_loss: 0.5025 - val_acc: 0.7638\n"
     ]
    }
   ],
   "source": [
    "model0 = start_out()\n",
    "model0.fit(train_x,train_y,batch_size=50,epochs=20,verbose=1,validation_data=(test_x,test_y))\n",
    "model0.save('model0_motoddata.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import os\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "          rotation_range=0.2,\n",
    "          width_shift_range=0.2,\n",
    "          height_shift_range=0.2,\n",
    "          shear_range=0.2,\n",
    "          zoom_range=0.2,\n",
    "          horizontal_flip=True,\n",
    "          fill_mode='nearest')\n",
    "\n",
    "\n",
    "prefix = \"C:/Users/xzh/Documents/Course_related/Big Data/Final Poject/input/\"\n",
    "old_doc = prefix + 'train/'\n",
    "new_doc = prefix + 'augmentation'\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images and saves the results to the `preview/` directory\n",
    "def generate_k_aug(file_path,to_path,k,prefix):\n",
    "    img = load_img(file_path)  \n",
    "    x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "    x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150 )   \n",
    "    i = 0\n",
    "    for batch in datagen.flow(x,batch_size=1,\n",
    "                            save_to_dir=to_path,#the srore path of the picture\n",
    "                            save_prefix=prefix,\n",
    "                            save_format='jpg'):\n",
    "        i += 1\n",
    "        if i > k: \n",
    "             break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "# Add 3 new photo for each photo in original train_x and save all of them to new_doc\n",
    "print(len(train_images))\n",
    "if os.path.exists(new_doc):\n",
    "    shutil.rmtree(new_doc)\n",
    "os.mkdir(new_doc)\n",
    "k=0\n",
    "for f in train_images:\n",
    "    k+=1\n",
    "    if k%1000==0:\n",
    "        print(k)\n",
    "    prefix = f\n",
    "    generate_k_aug(old_doc+f,new_doc,3,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform the photo of new_doc into array style and add it into train_x and train_y\n",
    "def get_new_data(new_doc):\n",
    "    train_images = os.listdir(new_doc)\n",
    "    train_x = prep_data(train_images,new_doc+'/')\n",
    "    train_y = [1 if x[:3]=='dog' else 0 for x in train_images]\n",
    "    train_y = to_categorical(train_y)\n",
    "    return train_x,train_y\n",
    "new_train_x,new_train_y = get_new_data(new_doc)\n",
    "#train_x+=new_train_x\n",
    "#train_y+=new_train_y\n",
    "#print('train shape:',train_x.shape,train_y.shape)\n",
    "#print('test shape:',test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (60000, 64, 64, 3) (60000, 2)\n",
      "test shape: (5000, 64, 64, 3) (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "final_x = np.vstack((train_x,new_train_x))\n",
    "final_y = np.vstack((train_y,new_train_y))\n",
    "print('train shape:',final_x.shape,final_y.shape)\n",
    "print('test shape:',test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 5000 samples\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 572s 10ms/step - loss: 0.5068 - acc: 0.7509 - val_loss: 0.4507 - val_acc: 0.7910\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 559s 9ms/step - loss: 0.4898 - acc: 0.7612 - val_loss: 0.4399 - val_acc: 0.7996\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 561s 9ms/step - loss: 0.4738 - acc: 0.7751 - val_loss: 0.4580 - val_acc: 0.7860\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 560s 9ms/step - loss: 0.4569 - acc: 0.7821 - val_loss: 0.4152 - val_acc: 0.8068\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 559s 9ms/step - loss: 0.4415 - acc: 0.7909 - val_loss: 0.4074 - val_acc: 0.8170\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 565s 9ms/step - loss: 0.4252 - acc: 0.8024 - val_loss: 0.3948 - val_acc: 0.8236\n"
     ]
    }
   ],
   "source": [
    "#model0 = start_out()\n",
    "history = model0.fit(final_x,final_y,batch_size=50,epochs=6,verbose=1,validation_data=(test_x,test_y))\n",
    "model0.save('model0_mottoddata.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model0.fit(final_x,final_y,batch_size=50,epochs=6,verbose=1,validation_data=(test_x,test_y))\n",
    "model0.save('model0_mottoddata.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Ensemble Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate single model accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1 = load_model('model1.h5')\n",
    "model2 = load_model('model2.h5')\n",
    "model3 = load_model('model3.h5')\n",
    "model4 = load_model('model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (25000, 64, 64, 3)\n",
      "Y shape: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "left_perc = 1\n",
    "train_perc = 1\n",
    "\n",
    "train_images,test_images = get_data(left_perc,train_perc)\n",
    "total_x = prep_data(train_images,prefix_dir)\n",
    "total_y = [1 if x[:3]=='dog' else 0 for x in train_images]\n",
    "total_y = to_categorical(total_y)\n",
    "print('X shape:',total_x.shape)\n",
    "print('Y shape:',total_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for model1 is: \n",
      "     loss = 0.6027353811836242, acc = 0.66272\n",
      "\n",
      "The results for model2 is: \n",
      "     loss = 0.6930758922576904, acc = 0.51116\n",
      "\n",
      "The results for model3 is: \n",
      "     loss = 0.6582265794754029, acc = 0.62364\n",
      "\n",
      "The results for model4 is: \n",
      "     loss = 0.6563407592010498, acc = 0.63092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [model1,model2,model3,model4]\n",
    "def calculate_accuracy(models,data_x,data_y):\n",
    "    k=0\n",
    "    for m in models:\n",
    "        k+=1\n",
    "        results = m.evaluate(data_x,data_y,verbose=0)\n",
    "        print('The results for model{} is: \\n     loss = {}, acc = {}\\n'.format(k,results[0],results[1]))\n",
    "\n",
    "calculate_accuracy(models,total_x,total_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Ensemble Accuracy by different weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when weights =  [2, 1, 1, 1]\n",
      "ensemble accuracy =  0.66196 \n",
      "\n",
      "when weights =  [3, 1, 1, 1]\n",
      "ensemble accuracy =  0.66292 \n",
      "\n",
      "when weights =  [4, 1, 1, 1]\n",
      "ensemble accuracy =  0.66272 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_preds(models,data_x):\n",
    "    preds = []\n",
    "    for m in models:\n",
    "        result = m.predict_classes(data_x)\n",
    "        preds.append( to_categorical(result) )\n",
    "    return preds\n",
    "\n",
    "def preds_to_classes(preds,weights):\n",
    "    score = np.zeros(preds[0].shape)\n",
    "    for w,p in zip(weights,preds):\n",
    "        score+=w*p\n",
    "    classes = score.argmax(axis=1)\n",
    "    return classes\n",
    "\n",
    "def classes_to_accuracy(classes,data_y):\n",
    "    accuracy = sum(classes==np.array([np.where(x==1) for x in data_y]).flatten())/len(data_y)\n",
    "    return accuracy\n",
    "\n",
    "preds = get_preds(models,total_x)\n",
    "\n",
    "for weights in [[2,1,1,1],[3,1,1,1],[4,1,1,1]]:\n",
    "    print('when weights = ',weights)\n",
    "    classes = preds_to_classes(preds,weights)\n",
    "    accuracy = classes_to_accuracy(classes,total_y)\n",
    "    print('ensemble accuracy = ',accuracy,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
